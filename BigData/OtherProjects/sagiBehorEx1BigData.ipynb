{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load the dataset\n",
    "rdd = sc.textFile(\"GBvideos.csv\")\n",
    "rddRows = rdd.map(lambda line: line.split(\",\"))\n",
    "# remove the headrs row\n",
    "header = rddRows.first()\n",
    "rddData= rddRows.filter(lambda line: line != header).filter(lambda line: len(line)==16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Transform the data into RDD pair\n",
    "tupleData=rddData.map(lambda x:(x[0],list(x[1:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5(Part a) - sort the data by the Views index, for task 5\n",
    "def lineViews(line):\n",
    "    return line[7]\n",
    "    \n",
    "rddData = rddData.sortBy(lineViews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel_title_distinct count is :673\n",
      "category_id_distinct count is :16\n",
      "tags_distinct count is :915\n",
      "views_distinct count is :12195\n",
      "likes_distinct count is :11093\n"
     ]
    }
   ],
   "source": [
    "# 3 a - count the distinct values on 5 columns : \n",
    "#channel_title\n",
    "#category_id\n",
    "#tags\n",
    "#views\n",
    "#likes\n",
    "\n",
    "def specific_column_str(line,index):\n",
    "    field = line[index]\n",
    "    return field\n",
    "\n",
    "def specific_column_int(line,index):\n",
    "    field = int(line[index])\n",
    "    return field\n",
    "\n",
    "channel_title = rddData.map(lambda line : specific_column_str(line,3))\n",
    "channel_title_distinct = channel_title.distinct()\n",
    "print('channel_title_distinct count is :' + str(channel_title_distinct.count()))\n",
    "\n",
    "category_id = rddData.map(lambda line : specific_column_str(line,4))\n",
    "category_id_distinct = category_id.distinct()\n",
    "print('category_id_distinct count is :' + str(category_id_distinct.count()))\n",
    "\n",
    "tags = rddData.map(lambda line : specific_column_str(line,6))\n",
    "tags_distinct = tags.distinct()\n",
    "print('tags_distinct count is :' + str(tags_distinct .count()))\n",
    "\n",
    "views = rddData.map(lambda line : specific_column_int(line,7))\n",
    "views_distinct = views.distinct()\n",
    "print('views_distinct count is :' + str(views_distinct .count()))\n",
    "\n",
    "likes = rddData.map(lambda line : specific_column_int(line,8))\n",
    "likes_distinct = likes.distinct()\n",
    "print('likes_distinct count is :' + str(likes_distinct.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Normalize views and likes values\n",
    "# We do it here to use it becuase it usfull for for the histogram\n",
    "viewsMax = views.max()\n",
    "viewsMin = views.min()\n",
    "views = views.map(lambda view : (view-viewsMin)/(viewsMax-viewsMin))\n",
    "\n",
    "likesMax = likes.max()\n",
    "likesMin = likes.min()\n",
    "likes = likes.map(lambda like : (like-likesMin)/(likesMax-likesMin))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1], [12277, 0, 0, 0, 1])\n",
      "([0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1], [12277, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# 3 b - After we normalize the data to valued from 0 to 1 the histogram function seperate the data to 5\n",
    "# bins, each of the in size of 0.2 .\n",
    "print(views.histogram(5))\n",
    "print(likes.histogram(5))\n",
    "\n",
    "#Explention of the result:\n",
    "#in both of the likes and views we have one value that is significanty bigger then the others, which \n",
    "#made most of the data to belong to the first bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4  - Fill in bad values into the mean of the values.\n",
    "# Here we are working on likes and views of many trending you tube video\n",
    "# so i think the mean value here fit best.\n",
    "\n",
    "def changeZeroTOMean(value, meanValue):\n",
    "    if value == 0:\n",
    "        return meanValue\n",
    "    else:\n",
    "        return value\n",
    "    \n",
    "likesMean = likes.mean()\n",
    "likes = likes.map(lambda line: changeZeroTOMean(line,likesMean))\n",
    "\n",
    "viewsMean = views.mean()\n",
    "views = views.map(lambda line: changeZeroTOMean(line,viewsMean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5  (Part b) - Before we sorted all the data by Views.\n",
    "#The data we work on is you tube videos, and this data usually tested  by views.\n",
    "#After we sort the data we give the categorical values numerical values when the loest value given to the \n",
    "#category with the less views.\n",
    "\n",
    "titleDict={}\n",
    "tagsDict={}\n",
    "def getNumValueTitle(category):\n",
    "    if category not in titleDict:\n",
    "        titleDict[category] = len(titleDict)\n",
    "    return titleDict[category] \n",
    "def getNumValueTags(category):\n",
    "    if category not in tagsDict:\n",
    "        tagsDict[category] = len(tagsDict)\n",
    "    return tagsDict[category] \n",
    "\n",
    "    \n",
    "\n",
    "channel_title_numerical = channel_title.map(lambda title:getNumValueTitle(title))\n",
    "channel_title_numerical.take(3)\n",
    "\n",
    "tags_numerical = tags.map(lambda tags:getNumValueTags(tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Here we create from each timestamp, that represent the publish time of the video,\n",
    "# a RDD of datetime, and from it we create RDD to the hours the days and the months.\n",
    "from datetime import datetime\n",
    "timestamp = rddData.map(lambda line : specific_column_str(line,5))\n",
    "\n",
    "dates = timestamp.map(lambda timestamp:datetime.strptime(timestamp,'%Y-%m-%dT%H:%M:%S.%fZ'))\n",
    "hours = dates.map(lambda date: date.hour)\n",
    "days = dates.map(lambda date: date.day)\n",
    "months = dates.map(lambda date: date.month)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 8  a - Here we tokenize the 'description' column from the data.\n",
    "#To do it we used Tokenizer Which is tool in Spark ML \n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "descriptionsDF = rddRows.map(lambda line : specific_column_str(line,15)).map(lambda x: x.split(\" \")).toDF([\"description\"])\n",
    "tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(descriptionsDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[description: string, words: array<string>, filtered: array<string>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8  b - Here we removed all the stop words from the data.\n",
    "#To do is we used StopWordsRemover which is tool in Spark ML\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "remover.transform(tokenized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
