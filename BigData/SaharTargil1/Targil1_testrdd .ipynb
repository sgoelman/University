{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TARGIL No. 1 \n",
    "# By Sahar Goelman\n",
    "# selected database: rasturants.csv\n",
    "#TODO: add link to the resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validate that the database file exists otherwise break\n",
    "if  not os.path.isfile(\"resturants.csv\"):\n",
    "    print(\"please add resturants.csv to :\"+os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: business_id,business_name,business_address,business_city,business_state,business_postal_code,business_latitude,business_longitude,business_location,business_phone_number,inspection_id,inspection_date,inspection_score,inspection_type,violation_id,violation_description,risk_category,rand_code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows loaded: 53577\n"
     ]
    }
   ],
   "source": [
    "#question (1) loading the database\n",
    "csv = sc.textFile(\"resturants.csv\")\n",
    "\n",
    "csv_header = csv.first() \n",
    "print(\"Column names:\",format(csv_header))\n",
    "csv_data = csv.filter(lambda line : line != csv_header) #remove csv headers from data\n",
    "print(\"Number of rows loaded: {}\".format(csv.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FiveHeaders:\n",
    "    def __init__(self):\n",
    "        self.values = []\n",
    "        self.csv_header = dict()\n",
    "    def add_headers(self, key, val):\n",
    "        self.csv_header[key] = val\n",
    "    def get_header(self, key):\n",
    "        return self.csv_header[key]\n",
    "    def get_all_values(self):\n",
    "        self.values = list(self.csv_header.values())\n",
    "        return self.values\n",
    "    def get_index_from_key(self, key):\n",
    "        index_in_header = self.get_header(key)\n",
    "        self.values.index(index_in_header)\n",
    "        return self.values.index(index_in_header)\n",
    "\n",
    "\n",
    "split_headers= csv_header.split(\",\")\n",
    "fiveHeaders = FiveHeaders()\n",
    "headers = ['business_id'] #, 'business_id','business_id','business_id','business_id']\n",
    "# headers = ['inspection_score', 'business_id','inspection_id','business_postal_code', 'business_phone_number']\n",
    "for header in headers: \n",
    "     fiveHeaders.add_headers(header, split_headers.index(header))\n",
    "\n",
    "# fiveHeaders.get_header('inspection_score')\n",
    "fiveHeaders.get_all_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added unique id at column 0:\nfirst=: (0, ['39810', 'Cherry Blossom Bakery', '844 Clement St', 'San Francisco', 'CA', '94118', '37.782778', '-122.468341', '\"{\\'latitude\\': \\'37.782778\\'', \" 'needs_recoding': False\", ' \\'human_address\\': \\'{\"\"address\"\":\"\"\"\"', '\"\"city\"\":\"\"\"\"', '\"\"state\"\":\"\"\"\"', '\"\"zip\"\":\"\"\"\"}\\'', ' \\'longitude\\': \\'-122.468341\\'}\"', '', '39810_20160308', '2016-03-08T00:00:00', '77', 'Routine - Unscheduled', '39810_20160308_103124', 'Inadequately cleaned or sanitized food contact surfaces', 'Moderate Risk', '94119'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added unique id at column 0:\nfirst[1]: ['39810', 'Cherry Blossom Bakery', '844 Clement St', 'San Francisco', 'CA', '94118', '37.782778', '-122.468341', '\"{\\'latitude\\': \\'37.782778\\'', \" 'needs_recoding': False\", ' \\'human_address\\': \\'{\"\"address\"\":\"\"\"\"', '\"\"city\"\":\"\"\"\"', '\"\"state\"\":\"\"\"\"', '\"\"zip\"\":\"\"\"\"}\\'', ' \\'longitude\\': \\'-122.468341\\'}\"', '', '39810_20160308', '2016-03-08T00:00:00', '77', 'Routine - Unscheduled', '39810_20160308_103124', 'Inadequately cleaned or sanitized food contact surfaces', 'Moderate Risk', '94119']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added unique id at column 0:\nfirst[1][1]: Cherry Blossom Bakery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in dataBase: 53576\n"
     ]
    }
   ],
   "source": [
    "#problem number (2):\n",
    "\n",
    "#RDD is assumed to contain an (unique) index column at position 0\n",
    "def get_RDD_row_by_index(rdd, index=0):\n",
    "    nrows = rdd.count()\n",
    "    if index < nrows:\n",
    "        return rdd.filter(lambda kv: kv[0] == index)\n",
    "    return None\n",
    "\n",
    "#RDD is assumed to contain the requested col_nama\n",
    "def get_RDD_col_values(rdd, col_name):\n",
    "    col_index = fv.get_index_from_key(col_name)\n",
    "    return rdd.map(lambda line: line[1][col_index])\n",
    "\n",
    "\n",
    "#add a unique_id and split csv using commas\n",
    "baseData = csv_data.zipWithIndex().map(lambda line: (line[-1],line[:-1][0].split(\",\")))\n",
    "print(\"Added unique id at column 0:\\nfirst=: {}\".format(baseData.first()))\n",
    "print(\"Added unique id at column 0:\\nfirst[1]: {}\".format(baseData.first()[1]))\n",
    "print(\"Added unique id at column 0:\\nfirst[1][1]: {}\".format(baseData.first()[1][1]))\n",
    "#test our method to get the n'th row from RDD\n",
    "#print(\"take the {}th row from Data: {}\".format(75, get_RDD_row_by_index(baseData,75).first()))\n",
    "print('number of rows in dataBase: {}'.format(baseData.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header indexes are:[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd[100][0] = 63940\n"
     ]
    }
   ],
   "source": [
    "#problem number (3.a): unique values of the 5 columns \n",
    "#RDD is assumed to contain the requested col_name\n",
    "#row 0 is the first row\n",
    "def get_RDD_col_value(rdd, row_index, col_name):\n",
    "    print (\"header indexes are:{}\".format(fiveHeaders.get_all_values()))\n",
    "    col_index = fiveHeaders.get_index_from_key(col_name)\n",
    "    row = get_RDD_row_by_index(rdd, row_index)\n",
    "    val  = row.first()[1][col_index]\n",
    "    print (\"rdd[{}][{}] = {}\".format(row_index, col_index,val))\n",
    "\n",
    "\n",
    "get_RDD_col_value(baseData, 100, 'business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering data\n<__main__.Data_Filter object at 0x0000015B058D3550>\nend filtering data\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "#allows counting only numerical columns\n",
    "class Data_Filter():\n",
    "    def __init__(self, header_name):\n",
    "        self.col_indexes = header_name.get_all_values()\n",
    "        self.build_line = self._builder(self.list_creator)\n",
    "        self.build_filter = self._builder(self.create_filter_is_num)\n",
    "        \n",
    "    def _builder(self, func):\n",
    "        return func(self.col_indexes)\n",
    "\n",
    "    def number(self, s):\n",
    "        ret = None\n",
    "        try:\n",
    "            ret = int(s)\n",
    "        except ValueError:\n",
    "            ret = float(s)\n",
    "        finally:\n",
    "            return ret\n",
    "\n",
    "    def is_not_number(self, s):\n",
    "        ret = False\n",
    "        try:\n",
    "            #print(\"value = \".format(s))\n",
    "            if (math.isnan(self.number(s))):  #not a number\n",
    "                ret = True\n",
    "        except:\n",
    "            print(\"exception in is_not_number, s = {}\".format(s))\n",
    "            ret = True\n",
    "        finally:\n",
    "            return ret\n",
    "\n",
    "    def list_creator(self, col_indexes):\n",
    "        def get_unique_line(_line, unique_id, append_num=True):\n",
    "            items = []\n",
    "            for col in col_indexes:\n",
    "                if append_num:\n",
    "                    item = self.number(_line[col])\n",
    "                else:\n",
    "                    item = _line[col]\n",
    "                items.append(item)\n",
    "            return (unique_id, items)\n",
    "        return get_unique_line\n",
    "\n",
    "\n",
    "    def create_filter_is_num(self, col_indexes):\n",
    "\n",
    "        def check_is_number(_line):\n",
    "            ret = True\n",
    "            for col in col_indexes:\n",
    "                try:\n",
    "                    #print (\"_line = {}\".format(_line))\n",
    "                    #print (\"value[{}]: {}\".format(col, _line[col]))\n",
    "                    if (math.isnan(self.number(_line[col]))):\n",
    "                        ret = False\n",
    "                except:\n",
    "                    #TODO\n",
    "                    print('create_filter_is_num caught an exception\\n _line[{}]={}'.format(col, _line[col]))\n",
    "                    ret = False\n",
    "            return ret\n",
    "        return check_is_number\n",
    "\n",
    "print (\"filtering data\")\n",
    "filtered_data = Data_Filter(fiveHeaders)\n",
    "print(filtered_data)\n",
    "print(\"end filtering data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of col_names[business_id] = 0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for col_name in headers: \n",
    "    print(\"index of col_names[{}] = {}\".format(col_name,filtered_data.col_indexes[i]))\n",
    "    i = i + 1\n",
    "    \n",
    "filteredByColumn = baseData.filter(lambda line: filtered_data.build_filter(line[1])).map(lambda line: filtered_data.build_line(line[1], line[0]))\n",
    "\n",
    "def count_distinct(filter_by_column, headerName, key):\n",
    "    res =  filter_by_column.map(lambda pair: pair[1][headerName.get_index_from_key(key)]).distinct().count()\n",
    "    return res\n",
    "def print_distinct(col_name):\n",
    "        cnt = count_distinct(filteredByColumn, fiveHeaders, col_name)\n",
    "        print('Distint values of col: {} = {}'.format(col_name, cnt))\n",
    "        return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distint values of col: business_id = 6155\n"
     ]
    }
   ],
   "source": [
    "#Question 3/a\n",
    "col_distinct_counts = {}\n",
    "for header in headers:\n",
    "    res = print_distinct(header)\n",
    "    col_distinct_counts[header] = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram of column:business_id\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1368x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Utility functions to plot histogram\n",
    "class Histogram():\n",
    "\n",
    "    def show(self, col_name, color):\n",
    "        obj = self.countAggregateByKey(col_name)\n",
    "        self.plot(obj,color)\n",
    "    \n",
    "    def countAggregateByKey(self, col_name):\n",
    "        aggragateByKey = col_name.countByValue() \n",
    "        keys  = aggragateByKey.keys()\n",
    "        x_axis = np.array(sorted(keys))\n",
    "        y_axis = np.array([aggragateByKey.get(key) for key in x_axis])\n",
    "        return self.buildObj(x_axis,y_axis)\n",
    "    \n",
    "    def buildObj(self, x_axis,y_axis):\n",
    "        res = dict()\n",
    "        res['x'] = {\n",
    "            'pos': np.arange(len(x_axis)),\n",
    "            'x_axis':x_axis\n",
    "        }\n",
    "        res['y']={\n",
    "            'y_axis':y_axis\n",
    "        }\n",
    "        return res\n",
    "    \n",
    "    def plot(self, obj,color):\n",
    "        y_axis = obj['y']['y_axis']\n",
    "        pos = obj['x']['pos']\n",
    "        x_axis = obj['x']['x_axis']\n",
    "\n",
    "        ax = plt.axes()\n",
    "        ax.set_xticks(pos)\n",
    "        ax.set_xticklabels(x_axis)\n",
    "\n",
    "        plt.bar(pos, y_axis, 0.5, color=color)\n",
    "        plt.xticks(size=16)\n",
    "\n",
    "        fig = matplotlib.pyplot.gcf()\n",
    "        fig.set_size_inches(19, 5)\n",
    "        plt.show()\n",
    "\n",
    "#Question 3/b drawing histograms of the 5 columns\n",
    "# get all indexes for a specific col name\n",
    "# we normalize the \"inpection_score\" variable so that \n",
    "def filterColumnByHeader(filteredByColumn,col_names,key):\n",
    "    return filteredByColumn.map(lambda pair: pair[1][col_names.get_index_from_key(key)])\n",
    "\n",
    "histogram_colors = ['red', 'green', 'blue', 'yellow', 'magenta']\n",
    "i = 0\n",
    "h = Histogram()\n",
    "for header in headers:\n",
    "    data = filterColumnByHeader(filteredByColumn,fiveHeaders,header)#\n",
    "    print (\"Histogram of column:{}\".format(header))\n",
    "    h.show(data, histogram_colors[i])\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "#Utility functions \n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRecoveryModel:\n",
    "    def __init__(self,v,sigma,median):\n",
    "        items = dict()\n",
    "        items['normal'] = {'mu':v[0]/v[1],'sigma':sigma }\n",
    "        items['median'] = median\n",
    "        self.items = items\n",
    "    def getByType(self,_type):\n",
    "        return self.items[_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRecoveryService():\n",
    "    def __init__(self, baseData, _fiveHeaders,dataRecoveryModel):\n",
    "        self.fiveHeaders = _fiveHeaders\n",
    "        self.items = dict()\n",
    "        self.dataRecoveryModel1 =]\\]\n",
    "        #hd = self.hd = Data_Filter(fiveHeaders)\n",
    "        hd = self.hd = Data_Filter(_fiveHeaders)\n",
    "        self.shrinkTableAndDarity = baseData.map(lambda line: hd.build_line(line[1], line[0], False))\n",
    "        self.filteredByColumn = baseData.filter(lambda line: hd.build_filter(line[1])).map(\n",
    "            lambda line: hd.build_line(line[1], line[0]))\n",
    "\n",
    "    def fixByType(self, typeFixer, cloumnName):\n",
    "        if typeFixer == 'mean':\n",
    "            metadata = self.dataRecoveryModel.getByType('normal')\n",
    "            return metadata['mu']\n",
    "        elif typeFixer == 'median':\n",
    "            return self.dataRecoveryModel.getByType('median')\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def findTheMidle(self, NumOfVal):\n",
    "        if NumOfVal % 2 != 0:\n",
    "            NumOfVal = NumOfVal + 1\n",
    "        return NumOfVal / 2\n",
    "\n",
    "    def calcDataRecoveryTypes(self, columnName):\n",
    "        hd = self.hd\n",
    "        fv = self.fiveHeaders\n",
    "        v = self.filteredByColumn.map(lambda pair: (pair[1][fv.get_index_from_key(columnName)], 1)).reduce(\n",
    "            lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "        n = self.findTheMidle(v[1])\n",
    "        median = self.filteredByColumn.map(lambda pair: (pair[1][fv.get_index_from_key(columnName)])) \\\n",
    "            .sortBy(lambda item: item).zipWithIndex().filter(lambda pair: pair[1] == n).collect()[0][0]\n",
    "        self.items[columnName] = DataRecoveryModel(v, 1, median)\n",
    "\n",
    "    def normelizeColumn(self, cloumnName, sigma):\n",
    "        fv = self.fiveHeaders\n",
    "        dataRecoveryModel = self.items[cloumnName]\n",
    "        metadata = dataRecoveryModel.getByType('normal')\n",
    "        mu = metadata['mu']\n",
    "        return self.filteredByColumn.map(\n",
    "          lambda pair: normalizeFixer(pair[1][fv.get_index_from_key(cloumnName)], mu, sigma))\n",
    "    \n",
    "    def fixCellsByType(self, typeFixer, columnName):\n",
    "        fv = self.fiveHeaders\n",
    "        hd = self.hd\n",
    "        fixedValue = self.fixByType(typeFixer, columnName)\n",
    "        return self.shrinkTableAndDarity.map(\n",
    "                lambda pair: fixCellsByTypePrivate(pair[1][fv.get_index_from_key(columnName)], fixedValue, hd))\n",
    "\n",
    "def normalizeFixer(x, mu, sigma):\n",
    "    return (x - mu)/sigma    \n",
    "    \n",
    "def fixCellsByTypePrivate(cell,fixedValue,hd):\n",
    "    isNotANumber = True\n",
    "    isNotANumber = hd.is_not_number(cell)\n",
    "    if isNotANumber :\n",
    "        cell = fixedValue\n",
    "    else: \n",
    "        cell = hd.num(cell)\n",
    "    return cell\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drawer():\n",
    "    @staticmethod\n",
    "    def countAggregateByKey(rddColumn):\n",
    "        aggragateByKey = rddColumn.countByValue() \n",
    "\n",
    "        keys  = aggragateByKey.keys()\n",
    "        x_axis = np.array(sorted(keys))\n",
    "        y_axis = np.array([aggragateByKey.get(key) for key in x_axis])\n",
    "        return Drawer.buildObj(x_axis,y_axis)\n",
    "    \n",
    "    @staticmethod\n",
    "    def buildObj(x_axis,y_axis):\n",
    "        res = dict()\n",
    "        res['x'] = {\n",
    "            'pos': np.arange(len(x_axis)),\n",
    "            'x_axis':x_axis\n",
    "        }\n",
    "        res['y']={\n",
    "            'y_axis':y_axis\n",
    "        }\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def _draw_histo(obj,color):\n",
    "        y_axis = obj['y']['y_axis']\n",
    "        pos = obj['x']['pos']\n",
    "        x_axis = obj['x']['x_axis']\n",
    "\n",
    "        ax = plt.axes()\n",
    "        ax.set_xticks(pos)\n",
    "        ax.set_xticklabels(x_axis)\n",
    "\n",
    "        plt.bar(pos, y_axis, 0.5, color=color)\n",
    "        plt.xticks(size=16)\n",
    "\n",
    "        fig = matplotlib.pyplot.gcf()\n",
    "        fig.set_size_inches(19, 5)\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def drow(rddColumn,color):\n",
    "        obj = Drawer.countAggregateByKey(rddColumn)\n",
    "        Drawer._draw_histo(obj,color)\n",
    "        \n",
    "def filterColumnByHeader(filteredByColumn,headerName,key):\n",
    "    return filteredByColumn.map(lambda pair: pair[1][headerName.get_index_from_key(key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 34, localhost, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:393)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:223)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:223)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:440)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:249)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:172)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:393)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:223)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:223)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:440)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:249)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:172)\r\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-3ad68d14d12c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mDRService\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataRecoveryService\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDRService\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalcDataRecoveryTypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'business_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDRService\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixCellsByType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"lightblue\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-4a5c551d4695>\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, col_name, color)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountAggregateByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-4a5c551d4695>\u001b[0m in \u001b[0;36mcountAggregateByKey\u001b[1;34m(self, col_name)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcountAggregateByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0maggragateByKey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountByValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mkeys\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0maggragateByKey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcountByValue\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1257\u001b[0m                 \u001b[0mm1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mm1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1259\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountPartition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmergeMaps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    840\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \"\"\"\n\u001b[0;32m    813\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 34, localhost, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:393)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:223)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:223)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:440)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:249)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:172)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.flushBuffer(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:393)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:223)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:223)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:223)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:440)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:249)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:172)\r\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "fv = FiveHeaders()\n",
    "fv.add_headers('business_id', 0)\n",
    "print(fv.get_all_values())\n",
    "hs = Histogram()\n",
    "DRService = DataRecoveryService(baseData, fv)\n",
    "\n",
    "print(DRService.calcDataRecoveryTypes('business_id'))\n",
    "hs.show(DRService.fixCellsByType('mean', col_name), \"lightblue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram of column:business_id\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 29.0 failed 1 times, most recent failure: Lost task 1.0 in stage 29.0 (TID 49, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"c:\\spark\\python\\pyspark\\rdd.py\", line 837, in func\n    initial = next(iterator)\n  File \"c:\\spark\\python\\pyspark\\rdd.py\", line 1251, in countPartition\n    for obj in iterator:\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-057b6d400d14>\", line 50, in <lambda>\n  File \"<ipython-input-28-057b6d400d14>\", line 61, in fixCellsByTypePrivate\nAttributeError: 'Data_Filter' object has no attribute 'num'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:332)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:471)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:454)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:286)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"c:\\spark\\python\\pyspark\\rdd.py\", line 837, in func\n    initial = next(iterator)\n  File \"c:\\spark\\python\\pyspark\\rdd.py\", line 1251, in countPartition\n    for obj in iterator:\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-057b6d400d14>\", line 50, in <lambda>\n  File \"<ipython-input-28-057b6d400d14>\", line 61, in fixCellsByTypePrivate\nAttributeError: 'Data_Filter' object has no attribute 'num'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:332)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:471)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:454)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:286)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-589e22f75fa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mDRService\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalcDataRecoveryTypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Histogram of column:{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDRService\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixCellsByType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistogram_colors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-4a5c551d4695>\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, col_name, color)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountAggregateByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-4a5c551d4695>\u001b[0m in \u001b[0;36mcountAggregateByKey\u001b[1;34m(self, col_name)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcountAggregateByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0maggragateByKey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountByValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mkeys\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0maggragateByKey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mx_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcountByValue\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1257\u001b[0m                 \u001b[0mm1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mm1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1259\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountPartition\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmergeMaps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    840\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    844\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \"\"\"\n\u001b[0;32m    813\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 29.0 failed 1 times, most recent failure: Lost task 1.0 in stage 29.0 (TID 49, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"c:\\spark\\python\\pyspark\\rdd.py\", line 837, in func\n    initial = next(iterator)\n  File \"c:\\spark\\python\\pyspark\\rdd.py\", line 1251, in countPartition\n    for obj in iterator:\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-057b6d400d14>\", line 50, in <lambda>\n  File \"<ipython-input-28-057b6d400d14>\", line 61, in fixCellsByTypePrivate\nAttributeError: 'Data_Filter' object has no attribute 'num'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:332)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:471)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:454)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:286)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:165)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 253, in main\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 248, in process\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 379, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"c:\\spark\\python\\pyspark\\rdd.py\", line 837, in func\n    initial = next(iterator)\n  File \"c:\\spark\\python\\pyspark\\rdd.py\", line 1251, in countPartition\n    for obj in iterator:\n  File \"c:\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-28-057b6d400d14>\", line 50, in <lambda>\n  File \"<ipython-input-28-057b6d400d14>\", line 61, in fixCellsByTypePrivate\nAttributeError: 'Data_Filter' object has no attribute 'num'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:332)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:471)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:454)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:286)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "fiveHeaders.add_headers('business_id', 0)\n",
    "print(fiveHeaders.get_all_values())\n",
    "\n",
    "#DRService = DataRecoveryService(baseData, fiveHeaders)\n",
    "DRService = DataRecoveryService(baseData, fv)\n",
    "\n",
    "hs = Histogram()\n",
    "i = 0\n",
    "for col_name in ['business_id']: #headers:\n",
    "    DRService.calcDataRecoveryTypes(col_name)\n",
    "    print(\"Histogram of column:{}\".format(col_name))\n",
    "    hs.show(DRService.fixCellsByType('mean', col_name), histogram_colors[i])\n",
    "    i = i + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
